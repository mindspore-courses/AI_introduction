{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e927a647",
   "metadata": {},
   "source": [
    "# 基于BERT的自动问答\n",
    "本实验基于MindSpore2.0,在启智平台上运行，使用的数据集是squad数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c0340",
   "metadata": {},
   "source": [
    "### 1.实验目的\n",
    "得益于深度学习模型如Bert的不断发展，使得人工智能在阅读理解问答任务上表现得越来越出色。本实验将使用BERT算法实现一个基于阅读理解任务的问答系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ba2b6",
   "metadata": {},
   "source": [
    "### 2.BERT介绍\n",
    "BERT 全称为Bidirectional Encoder Representation from Transformers（来自Transformers的双向编码表示），谷歌发表的发的论文Pre-traning of Deep Bidirectional Transformers for Language Understanding中提出的一个面向自然语言处理任务的无监督预训练语言模型。是近年来自然语言处理领域公认的里程碑模型。\n",
    "BERT的创新在于Transformer Decoder（包含Masked Multi-Head Attention）作为提取器，并使用与之配套的掩码训练方法。虽然使用了双编码使得BERT不具有文本生成能力，但BERT在对输入文本的编码过程中，利用了每个词的所有上下文信息，与只能使用前序信息提取语义的单向编码器相比，BERT的语义信息提取能力更强。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a8270",
   "metadata": {},
   "source": [
    "### 3.实验环境\n",
    "本实验基于MindSpore2.0,在启智NPU平台上运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b321a4a",
   "metadata": {},
   "source": [
    "### 4.实验过程\n",
    "步骤1 下载数据和导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73170e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com:443/NLP/src.zip (223 kB)\n",
      "\n",
      "file_sizes: 100%|█████████████████████████████| 229k/229k [00:00<00:00, 779kB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from download import download\n",
    "\n",
    "url = \"https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com:443/NLP/src.zip\"\n",
    "\n",
    "download(url, \"./\", kind=\"zip\", replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4892500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com:443/NLP/data.zip (1.08 GB)\n",
      "\n",
      "file_sizes: 100%|██████████████████████████| 1.16G/1.16G [00:17<00:00, 67.1MB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from download import download\n",
    "\n",
    "url = \"https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com:443/NLP/data.zip\"\n",
    "\n",
    "download(url, \"./\", kind=\"zip\", replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409f9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "#import mindspore.common.dtype as mstype\n",
    "from mindspore import dtype as mstype\n",
    "#import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore.dataset.transforms as C\n",
    "\n",
    "\n",
    "#import mindspore.common.dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import context\n",
    "from mindspore import log as logger\n",
    "#from mindspore.nn.wrap.loss_scale import DynamicLossScaleUpdateCell\n",
    "from mindspore.nn import DynamicLossScaleUpdateCell\n",
    "#from mindspore.nn.optim import AdamWeightDecay, Lamb, Momentum\n",
    "from mindspore.nn import AdamWeightDecay, Lamb, Momentum\n",
    "#from mindspore.common.tensor import Tensor\n",
    "from mindspore import Tensor\n",
    "#from mindspore.train.model import Model\n",
    "#from mindspore.train.callback import CheckpointConfig, ModelCheckpoint, TimeMonitor\n",
    "from mindspore.train import Model, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "#from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "from src.bert_for_finetune import BertSquadCell, BertSquad\n",
    "from src.finetune_eval_config import optimizer_cfg\n",
    "from src.bert_model import BertConfig\n",
    "from src.utils import make_directory, LossCallBack, LoadNewestCkpt, BertLearningRate\n",
    "\n",
    "\n",
    "\n",
    "_cur_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b2442",
   "metadata": {},
   "source": [
    "步骤2\t运行环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7ad6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")  #, device_id=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63e884",
   "metadata": {},
   "source": [
    "步骤3 定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da09381",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_opt = edict({\n",
    "    \"device_target\":\"Ascend\",\n",
    "    \"do_train\":\"true\",\n",
    "    \"do_eval\":\"true\",\n",
    "    \"epoch_num\":3,\n",
    "    \"num_class\":2,\n",
    "    \"train_data_shuffle\":\"false\",\n",
    "    \"eval_data_shuffle\":\"false\",\n",
    "    \"train_batch_size\":32,\n",
    "    \"eval_batch_size\":1,\n",
    "    \"vocab_file_path\":\"./data/vocab_bert_large_en.txt\",\n",
    "    \"save_finetune_checkpoint_path\":\"./ckpt/\",\n",
    "    \"load_pretrain_checkpoint_path\":\"./data/pretrain_ckpt/bert_base.ckpt\",\n",
    "    \"load_finetune_checkpoint_path\":\"./ckpt/squad-3_2745.ckpt\",\n",
    "    \"train_data_file_path\":\"./data/train.tf_record\",\n",
    "    \"eval_json_path\":\"./data/dev-v1.1.json\",\n",
    "    \"schema_file_path\":\"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1f8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_net_cfg = BertConfig(\n",
    "    seq_length=384,\n",
    "    vocab_size=21128,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    use_relative_positions=False,\n",
    "    dtype=mstype.float32,\n",
    "    compute_type=mstype.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b38ba3",
   "metadata": {},
   "source": [
    "步骤4\t数据预览\n",
    "    使用json模块导入训练数据train-v1.1.json："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f849a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'qas': [{'answers': [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}], 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'id': '5733be284776f41900661182'}, {'answers': [{'answer_start': 188, 'text': 'a copper statue of Christ'}], 'question': 'What is in front of the Notre Dame Main Building?', 'id': '5733be284776f4190066117f'}, {'answers': [{'answer_start': 279, 'text': 'the Main Building'}], 'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?', 'id': '5733be284776f41900661180'}, {'answers': [{'answer_start': 381, 'text': 'a Marian place of prayer and reflection'}], 'question': 'What is the Grotto at Notre Dame?', 'id': '5733be284776f41900661181'}, {'answers': [{'answer_start': 92, 'text': 'a golden statue of the Virgin Mary'}], 'question': 'What sits on top of the Main Building at Notre Dame?', 'id': '5733be284776f4190066117e'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.load(open(\"./data/train-v1.1.json\"))\n",
    "print(data[\"data\"][0][\"paragraphs\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08eab80",
   "metadata": {},
   "source": [
    "从train-v1.1.json的数据内容分析来看，通过json模块载入后形成一个词典，数据集中在”data”键下。data[\"data\"][0][\"paragraphs\"][0]中第一个[0]代表第一个元素，”paragraph”代表段落。上面的输出结果为一个段落的内容。\n",
    "我们可以看到，这里有一段文本”context”，同时有”qas”问答部分将答案与问题对应。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e47ef2",
   "metadata": {},
   "source": [
    "步骤5 定义数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee58816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_squad(data_features):\n",
    "    for feature in data_features:\n",
    "        yield (feature.input_ids, feature.input_mask, \n",
    "               feature.segment_ids, feature.unique_id)\n",
    "        \n",
    "def create_squad_dataset(batch_size=1, repeat_count=1, data_file_path=None, \n",
    "                         schema_file_path=None,is_training=True, do_shuffle=True):\n",
    "    \"\"\"create finetune or evaluation dataset\"\"\"\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "    if is_training:\n",
    "        data_set = ds.TFRecordDataset([data_file_path], \n",
    "                                      schema_file_path if schema_file_path != \"\" else None,\n",
    "                                      columns_list=[\"input_ids\", \"input_mask\", \n",
    "                                            \"segment_ids\", \"start_positions\",\n",
    "                                            \"end_positions\", \"unique_ids\", \"is_impossible\"],\n",
    "                                      shuffle=do_shuffle)\n",
    "        data_set = data_set.map(operations=type_cast_op, input_columns=\"start_positions\")\n",
    "        data_set = data_set.map(operations=type_cast_op, input_columns=\"end_positions\")\n",
    "    else:\n",
    "        data_set = ds.GeneratorDataset(generator_squad(data_file_path), \n",
    "                                       shuffle=do_shuffle,column_names=[\"input_ids\", \n",
    "                                        \"input_mask\", \"segment_ids\", \"unique_ids\"])\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"segment_ids\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"input_mask\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"input_ids\")\n",
    "    data_set = data_set.map(operations=type_cast_op, input_columns=\"unique_ids\")\n",
    "    #data_set = data_set.repeat(repeat_count)      #去掉repeat\n",
    "    # apply batch operations\n",
    "    data_set = data_set.batch(batch_size, drop_remainder=True)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb2d9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Tensor(shape=[32, 384], dtype=Int32, value=\n",
       " [[ 101, 1999, 2054 ...    0,    0,    0],\n",
       "  [ 101, 2040, 2020 ...    0,    0,    0],\n",
       "  [ 101, 2054, 2003 ...    0,    0,    0],\n",
       "  ...\n",
       "  [ 101, 2043, 2001 ...    0,    0,    0],\n",
       "  [ 101, 2054, 2173 ...    0,    0,    0],\n",
       "  [ 101, 2054, 2846 ...    0,    0,    0]]),\n",
       " 'input_mask': Tensor(shape=[32, 384], dtype=Int32, value=\n",
       " [[1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  ...\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0]]),\n",
       " 'segment_ids': Tensor(shape=[32, 384], dtype=Int32, value=\n",
       " [[0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  ...\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0],\n",
       "  [0, 0, 0 ... 0, 0, 0]]),\n",
       " 'start_positions': Tensor(shape=[32, 1], dtype=Int32, value=\n",
       " [[22],\n",
       "  [51],\n",
       "  [48],\n",
       "  ...\n",
       "  [31],\n",
       "  [20],\n",
       "  [76]]),\n",
       " 'end_positions': Tensor(shape=[32, 1], dtype=Int32, value=\n",
       " [[22],\n",
       "  [58],\n",
       "  [51],\n",
       "  ...\n",
       "  [31],\n",
       "  [20],\n",
       "  [79]]),\n",
       " 'unique_ids': Tensor(shape=[32, 1], dtype=Int32, value=\n",
       " [[1000000000],\n",
       "  [1000000001],\n",
       "  [1000000002],\n",
       "  ...\n",
       "  [1000000029],\n",
       "  [1000000030],\n",
       "  [1000000031]]),\n",
       " 'is_impossible': Tensor(shape=[32, 1], dtype=Int64, value=\n",
       " [[0],\n",
       "  [0],\n",
       "  [0],\n",
       "  ...\n",
       "  [0],\n",
       "  [0],\n",
       "  [0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = create_squad_dataset(batch_size=args_opt.train_batch_size, repeat_count=1,\n",
    "                              data_file_path=args_opt.train_data_file_path,\n",
    "                              schema_file_path=args_opt.schema_file_path,\n",
    "                              do_shuffle=(args_opt.train_data_shuffle.lower() == \"true\"))\n",
    "item =train_ds.create_dict_iterator()\n",
    "next(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f370f",
   "metadata": {},
   "source": [
    "模型的输入包括：input_ids, input_mask, segment_ids, start_positions, end_positions, unique_ids, is_impossible。分别代表输入的id，输入句子的有效标记，输入属于那一句话，答案的开始位置、结束位置，特征id，是否有答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261b67f",
   "metadata": {},
   "source": [
    "步骤6 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f703f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(dataset=None, network=None, load_checkpoint_path=\"\", save_checkpoint_path=\"\", epoch_num=1):\n",
    "    \"\"\" do train \"\"\"\n",
    "    if load_checkpoint_path == \"\":\n",
    "        raise ValueError(\"Pretrain model missed, finetune task must load pretrain model!\")\n",
    "    steps_per_epoch = dataset.get_dataset_size()\n",
    "    \n",
    "    # 优化器\n",
    "    lr_schedule = BertLearningRate(learning_rate=optimizer_cfg.AdamWeightDecay.learning_rate,\n",
    "                                   end_learning_rate=optimizer_cfg.AdamWeightDecay.end_learning_rate,\n",
    "                                   warmup_steps=int(steps_per_epoch * epoch_num * 0.1),\n",
    "                                   decay_steps=steps_per_epoch * epoch_num,\n",
    "                                   power=optimizer_cfg.AdamWeightDecay.power)\n",
    "    params = network.trainable_params()\n",
    "    decay_params = list(filter(optimizer_cfg.AdamWeightDecay.decay_filter, params))\n",
    "    other_params = list(filter(lambda x: not optimizer_cfg.AdamWeightDecay.decay_filter(x), params))\n",
    "    group_params = [{'params': decay_params, 'weight_decay': optimizer_cfg.AdamWeightDecay.weight_decay},\n",
    "                    {'params': other_params, 'weight_decay': 0.0}]\n",
    "\n",
    "    optimizer = AdamWeightDecay(group_params, lr_schedule, eps=optimizer_cfg.AdamWeightDecay.eps)\n",
    "        \n",
    "    # 导入预先训练好的模型\n",
    "    ckpt_config = CheckpointConfig(save_checkpoint_steps=steps_per_epoch, keep_checkpoint_max=1)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=\"squad\",\n",
    "                                 directory=None if save_checkpoint_path == \"\" else save_checkpoint_path,\n",
    "                                 config=ckpt_config)\n",
    "    param_dict = load_checkpoint(load_checkpoint_path)\n",
    "    load_param_into_net(network, param_dict)\n",
    "\n",
    "    update_cell = DynamicLossScaleUpdateCell(loss_scale_value=2**32, scale_factor=2, scale_window=1000)\n",
    "    netwithgrads = BertSquadCell(network, optimizer=optimizer, scale_update_cell=update_cell)\n",
    "    model = Model(netwithgrads)\n",
    "    \n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=2745, keep_checkpoint_max=10)\n",
    "    ckpoint = ModelCheckpoint(prefix=\"CKP\", config=config_ck)\n",
    "    \n",
    "    \n",
    "    #callbacks = [TimeMonitor(dataset.get_dataset_size()), LossCallBack(dataset.get_dataset_size()), ckpoint_cb]\n",
    "    \n",
    "    callbacks=[ckpoint, TimeMonitor(2745)]\n",
    "    \n",
    "    model.train(epoch_num, dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea8f29",
   "metadata": {},
   "source": [
    "步骤7 执行训练  \n",
    "加载bert_base.ckpt由于5个参数模型加载不上去，会出现警告，因此需要训练好一个ckpt文件CKP_1-3_2745.ckpt用来替换bert_base.ckpt，这样参数才能都加载上去，没有警告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af800d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch time: 501446.709 ms, per step time: 182.676 ms\n",
      "Train epoch time: 408718.344 ms, per step time: 148.896 ms\n",
      "Train epoch time: 403629.564 ms, per step time: 147.042 ms\n"
     ]
    }
   ],
   "source": [
    "#!export GLOG_v=1\n",
    "\n",
    "netwithloss = BertSquad(bert_net_cfg, True, 2, dropout_prob=0.1)\n",
    "\n",
    "train_ds = create_squad_dataset(batch_size=args_opt.train_batch_size, repeat_count=1,\n",
    "                          data_file_path=args_opt.train_data_file_path,\n",
    "                          schema_file_path=args_opt.schema_file_path,\n",
    "                          do_shuffle=(args_opt.train_data_shuffle.lower() == \"true\"))\n",
    "do_train(train_ds, netwithloss, args_opt.load_pretrain_checkpoint_path, args_opt.save_finetune_checkpoint_path, args_opt.epoch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b0a9b",
   "metadata": {},
   "source": [
    "步骤8 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b70f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(dataset=None, load_checkpoint_path=\"\", eval_batch_size=1):\n",
    "    # 加载训练参数、定义模型、定义输出结果变量\n",
    "    if load_checkpoint_path == \"\":\n",
    "        raise ValueError(\"Finetune model missed, evaluation task must load finetune model!\")\n",
    "    net = BertSquad(bert_net_cfg, False, 2)\n",
    "    net.set_train(False)\n",
    "    param_dict = load_checkpoint(load_checkpoint_path)\n",
    "    load_param_into_net(net, param_dict)\n",
    "    model = Model(net)\n",
    "    output = []\n",
    "    RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "    columns_list = [\"input_ids\", \"input_mask\", \"segment_ids\", \"unique_ids\"]\n",
    "    # 对评估数据集进行预测评估\n",
    "    for data in dataset.create_dict_iterator(num_epochs=1):\n",
    "        input_data = []\n",
    "        for i in columns_list:\n",
    "            input_data.append(data[i])\n",
    "        input_ids, input_mask, segment_ids, unique_ids = input_data\n",
    "        start_positions = Tensor([1], mstype.float32)\n",
    "        end_positions = Tensor([1], mstype.float32)\n",
    "        is_impossible = Tensor([1], mstype.float32)\n",
    "        logits = model.predict(input_ids, input_mask, segment_ids, start_positions,\n",
    "                               end_positions, unique_ids, is_impossible)\n",
    "        ids = logits[0].asnumpy()\n",
    "        start = logits[1].asnumpy()\n",
    "        end = logits[2].asnumpy()\n",
    "\n",
    "        for i in range(eval_batch_size):\n",
    "            unique_id = int(ids[i])\n",
    "            start_logits = [float(x) for x in start[i].flat]\n",
    "            end_logits = [float(x) for x in end[i].flat]\n",
    "            output.append(RawResult(\n",
    "                unique_id=unique_id,\n",
    "                start_logits=start_logits,\n",
    "                end_logits=end_logits))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610170ce",
   "metadata": {},
   "source": [
    "步骤9 评估模型效果\n",
    " 模型评估针对dev-v1.1.json中的数据，通过测试集评估训练效果，\n",
    "测试集数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "680fbfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"exact_match\": 41.83538315988647, \"f1\": 54.42541185741247}\n"
     ]
    }
   ],
   "source": [
    "from src import tokenization\n",
    "from src.create_squad_data import read_squad_examples, convert_examples_to_features\n",
    "from src.squad_get_predictions import write_predictions\n",
    "from src.squad_postprocess import SQuad_postprocess\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=args_opt.vocab_file_path, do_lower_case=True)\n",
    "eval_examples = read_squad_examples(args_opt.eval_json_path, False)\n",
    "eval_features = convert_examples_to_features(\n",
    "    examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=bert_net_cfg.seq_length,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=False,\n",
    "    output_fn=None,\n",
    "vocab_file=args_opt.vocab_file_path)\n",
    "eval_ds = create_squad_dataset(batch_size=args_opt.eval_batch_size, repeat_count=1,\n",
    "                          data_file_path=eval_features,\n",
    "                          schema_file_path=args_opt.schema_file_path, is_training=False,\n",
    "                          do_shuffle=(args_opt.eval_data_shuffle.lower() == \"true\"))\n",
    "\n",
    "outputs = do_eval(eval_ds, args_opt.load_finetune_checkpoint_path, args_opt.eval_batch_size)\n",
    "all_predictions = write_predictions(eval_examples, eval_features, outputs, 20, 30, True)\n",
    "SQuad_postprocess(args_opt.eval_json_path, all_predictions, output_metrics=\"output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b3a7f",
   "metadata": {},
   "source": [
    "步骤10 单例评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0f0fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Dog is the best friend of human.', 'qas': [{'answers': [{'answer_start': 0, 'text': 'Dog'}], 'question': 'Who is the best friend of human?', 'id': '56be4db0acb8001400a502ec'}]}\n",
      "OrderedDict([('56be4db0acb8001400a502ec', 'Dog')])\n"
     ]
    }
   ],
   "source": [
    "one_example_path = \"./data/my_example.json\"\n",
    "data = json.load(open(one_example_path))\n",
    "print(data[\"data\"][0][\"paragraphs\"][0])\n",
    "one_examples = read_squad_examples(one_example_path, False)\n",
    "one_features = convert_examples_to_features(\n",
    "    examples=one_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=bert_net_cfg.seq_length,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=False,\n",
    "    output_fn=None,\n",
    "    vocab_file=args_opt.vocab_file_path)\n",
    "one_ds = create_squad_dataset(batch_size=1, repeat_count=1,\n",
    "                          data_file_path=one_features,\n",
    "                          schema_file_path=args_opt.schema_file_path, is_training=False,\n",
    "                          do_shuffle=(args_opt.eval_data_shuffle.lower() == \"true\"))\n",
    "outputs = do_eval(one_ds, args_opt.load_finetune_checkpoint_path, args_opt.eval_batch_size)\n",
    "all_predictions = write_predictions(one_examples, one_features, outputs, 2, 30, True)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbe652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a328987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2a409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98194dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37073b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55c1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da517a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea14f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
